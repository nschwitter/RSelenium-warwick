---
title: "Web scraping with R"
author: Nicole Schwitter
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: yes
    toc_float: true
    toc_collapsed: true
---

# Scraping with rvest

Take a look at the movie [SpaceJam on IMDB](https://www.imdb.com/title/tt0117705/). We want to collect its reviews. 

We will first use the popular package `rvest` for scraping. We will load the package as well as `dplyr` for some additional data manipulation functions.

```{r}
library(rvest)
library(dplyr)
```

How do we go about scraping the reviews? Manually inspecting the site, we found where they are: [https://www.imdb.com/title/tt0117705/reviews]. We read this page into R. To do that, we need to tell R its URL which we first save into the new object `url`. Next, we parse the web page. We use the function `read_html()`.

```{r}
url <- "https://www.imdb.com/title/tt0117705/reviews"
spacejamrevhtml <- read_html(url)
```

`spacejamrevhtml` is a list containing the entire document, including the HTML formatting. If you do not anything about HTML, [W3Schools](https://www.w3schools.com) is a great resource for learning it. 

## Excursus: HTML and CSS

For web-scraping, we make use of the fact that web pages follow a standardised structure. We can tell R exactly which element of this structure we want to collect. 

Web pages have a head and a body. HTML documents consist of a tree of elements and text. Inside the body of a HTML page, tags can specify the elements to be displayed. 
Examples:

`<h1>This is a heading.</h1>`

`<p>This is a paragraph.</p><p>And this is the next.</p>`

In addition, HTML attributes can provide additional information about HTML elements. Attributes are included in the starting tag and come as name-value-pairs (attributename="attributevalue"). We often need to use attributes to specify what exactly we want to scrape. An example of an attribute are **classes**. They specify a class for an HTML element. Multiple HTML elements can share the same class. We often encounter classes because they are frequently used for consistent formatting of similar content. 

Example: `<p class="definitions">This is the text within the p of class definitions.</p> `

For an overview of further attributes, check [W3Schools](https://www.w3schools.com/html/html_attributes.asp).

How can we specify the tags and attributes of the information we are interested in? We use CSS selectors. CSS selectors are pattern matching rules which are originally used to determine which style and formatting rules apply to which elements. 

Examples: `p` refers to all paragraphs on a website. `.definitions` refers to all elements with the class definitions on a website. 

To learn more about CSS selectors, check [W3Schools](https://www.w3schools.com/cssref/css_selectors.php).

We can find the relevant element tags and attributes through our browser, by looking at the HTML source code of any website and by inspecting specific elements. There are also tools to make our life easier like ***SelectorGadget*** (check out its [webpage](https://selectorgadget.com/)). 


# The limits of rvest

What we want to extract from the scraped site is the user-written reviews: These are specific elements. 

The  method `html_elements()` allows the selection of such specific elements of the HTML code. The documentation of the `html_elements()`command reveals that we need CSS selectors (or XPath expressions) to specify what we want to select. 

We inspect the website to find out how we can access that information. 

You can use SelectorGadget to find the correct CSS selector. We assign the names to the new object `selected_elements` and then inspect the results. To get rid of the HTML tags, we use the `html_text()` command.

```{r}
selected_elements<-html_elements(spacejamrevhtml,".review-container")
selected_elements
reviews <- html_text(selected_elements)
head(reviews)
```

There we are - with a list of the 25 first reviews. Collecting more becomes more complex: The user reviews site is not a static but a dynamic site. To see more reviews, we need to click the ***load more***-button. This is what we need `RSelenium` for!

# Time to shine for RSelenium

We just collected user reviews of SpaceJam. However, we only collected the first 25 reviews. We then need to click the ***load more***-button to see more reviews.

We do that using the package `RSelenium`. Selenium allows driving a web browser natively the way a user would. There are different ways to run RSelenium. We will make use of the `rsDriver()` function it provides via the Webdriver manager package `wdman`. Because of different operating systems, browser versions and Selenium version, it can be fiddly to get this to run. When aiming for more replicable scraping and better platform stability, a recommended way is to use [Docker](https://www.docker.com/products/docker-desktop/). Docker is a free software for isolating applications using container virtualisation. Running a Docker container standardises the build across operating system. However, `rsDriver()` allows us to more easily follow along and see exactly what is happening. 

We now load in the `RSelenium` package and start with the setup of our driver. First, we need to download binaries, start the driver and get the client object. We will open a `chrome` browser and specify the version we are calling. The port identifies your browser. You cannot open multiple instances on the same port. If you get the error message that a port is already in use, just change the port number. 

```{r}
library(RSelenium)
driver <- rsDriver(browser = "chrome",chromever = "108.0.5359.71",port = 4444L)
browser <- driver$client
```

A new browser window opened up! You will now navigate it remotely. We will tell our browser to navigate to the page of user reviews for SpaceJam:

```{r}
browser$navigate("https://www.imdb.com/title/tt0117705/reviews")
```

The browser opened the website! We now want to tell the browser to click on the ***load more*** button. We do this by telling our browser to find the element and then to click on it. To find and element, we use the function `findElement()`. Again, we use CSS selectors to tell our browser that we are looking for the load more button. Once we find the load button, we assign it to the object `load_button`. We then click on it and wait 2 seconds so that the elements have some time to load using `Sys.sleep()`. Depending on the speed of your internet connection and the complexity of the website you are working with, you might want to wait shorter or longer than 2 seconds. 

```{r}
load_button <- browser$findElement(using = "css selector", "#load-more-trigger")
load_button$clickElement()
Sys.sleep(2) 
```

It is a little bit like magic: The browser clicked the button! We need to repeat this now. We know that 290 reviews have been written and 25 are displayed per page. Thus, there are 12 pages with information. We need to click the button 10 more times and can do so in a simple loop. Look at your browser and watch the magic happen!

```{r}
for (i in 1:10){
  load_button <- browser$findElement(using = "css selector", "#load-more-trigger")
  load_button$clickElement()
  Sys.sleep(2)
}
```

Now, all reviews are visible! Let us get to the HTML. We cannot use `rvest` right now, because we need to get the HTML from the currently displayed version that `RSelenium` navigated us to. `RSelenium` also has a function to get the HTML code of a site, `getPageSource()`. We use this function and save it in `reviewdata`.

```{r}
reviewdata <- browser$getPageSource()
```

Using `getPageSource()`, the source code is saved as a list. We extract the first element of the list and can then continue with methods from `rvest`.

```{r}
reviewdata <- reviewdata[[1]]

spacejamrevhtml_all <- read_html(reviewdata)
reviews_all <- html_text(html_elements(spacejamrevhtml_all, ".review-container"))
```

There you go - we collected all reviews!

Dynamic web pages are the time to shine for `RSelenium`. Next to clicking on elements, another popular application is to use it when sending text to form fields:

```{r}
browser$navigate("https://tfl.gov.uk/modes/tube/")

station <- browser$findElement(using = "css selector", "#Input")

station$sendKeysToElement(list("Paddington Underground Station"))
                               
go_button <- browser$findElement(using = "css selector", "#go-submit")
go_button$clickElement()
```

If you are interested in learning more about web-scraping, you can also take a look at my scripts and slides here: [https://github.com/nschwitter/webdata-warwick]